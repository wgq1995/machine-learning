# 凸函数
    凸函数曲面任意两点的连线，其上的任意一点都在这两点之间曲面的上方
    L(a * x + (1 - a) * y) <= a * L(x) + (1 - a) * L(y)

# 交叉熵
    L = -sum(yi * log(pre_yi)) , 每个样本的损失， 因为yi只在某一类上取1，所以是在最大化在该类上的概率

# 牛顿法
    应用： 求解方程整数解
    
# 随机梯度下降
    应用场景： 梯度下降会计算所有的数据，计算量大
    方法： 每次只计算一个随即样本的梯度，进行更新
    
# 小批量梯度下降
    方法： 每次计算一小批的数据， 进行更新
    
# 随机梯度下降存在的问题
    1. 山谷： 准确的方向应该是沿着山道向下，随机梯度下降会使路径在山壁上来回反弹，导致收敛不稳定或者收敛速度慢
    2. 鞍点： 鞍点处很平坦，各个方向的梯度近乎为0， 导致在不正确的地方早停
# 添加动量的梯度下降(Monentum)
    思路：
        每次更新参数的时候，考虑前几次更新的结果
    更新公式：
        v_t = a * v_t-1 + b * g_t
        w_t+1 = w_t - v_t

# 自适应的学习率AdaGrad
    思路：
        历史更新较少的梯度有更大的学习率，更新大的梯度有较小的学习率
    更新公式：
        w_t+1,i = w_t,i - r * g_t,i / sqrt(sum_0_t_(g_k,i*g_k,i + c))
        w_t+1,i 表示t+1时刻的参数向量的第i个参数
        g_k,i 表示k时刻的梯度向量g_k的第i个方向
        分母的形式表示退火过程，随着学习的进行，学习率逐渐下降，保证收敛
# 考虑梯度的自适应学习率（Adam）
    思路：
        综合上述两种更新算法，既考虑梯度，有自适应学习率
    更新公式：
        m_t = a * m_t-1 + (1-a) * g_t
        v_t = b * v_t-1 + (1-b) * g_t ** 2
        其中，a,b为衰减系数，m, v分别代表一阶矩和二阶矩
        w_t+1 = w_t - r * m_tt / sqrt(v_tt + c)
        其中，m_tt = m_t / (1-a_t), v_tt = v_t / (1-b_t),考虑了零初始状态的偏执矫正

# L1正则与稀疏性
    L1正则会使很多参数为0，起到特征选择的作用
    L1正则是矩形空间，L2正则是圆形空间
    根据求导解释：
        L1 原点左边是-C，右边是C，如果原参数大于0，那么损失就大，小于0，损失就小，最终容易到达0处
        L2 求梯度是w，那么w越大，损失越大，w越小，损失越小，所以可以减小w绝对值，并且均衡各个参数
