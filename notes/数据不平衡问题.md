# 问题提出
    一般正负样本相差一个数量级以上称为数据不平衡问题
# 基于数据的解决方案
## 上采样
    方法： 将数据量小的样本复制多份
    存在的问题： 数据集中出现很多重复样本
    解决方法： 
      1. 在生成新数据时，随机加入一些噪声
      2. SMOTE算法： 在样本量小的类中的每个样本x，从小类集合中随机选择一个它的k近邻样本y, 从x, y连线上选择一点作为新样本（根据采样倍率，重复若干次）
      3. Borderline-SMOTE： 进行k近邻选择时，只选择那些k近邻包含一半以上大众样本的小样本进行扩充，因为这些小样本可能是分类边界
      4. ADASYN: 根据小样本k近邻中大样本的比例决定该小样本的扩充个数， 扩充比例个数为 gi= G * x(i, k)/z， 其中x(i, k）为第i个小样本k近邻中大样本个数，z为所有小样本k近邻大样本个数，G为一共要扩充的个数
## 下采样
    方法： 从数量多的样本中选择一个子集
    存在的问题： 最终的模型只学到了原始数据的一部分
    解决方法：
      1. 简单集成： 在随即采样多次，每次训练一个模型，最终结果由多个模型决定，其中一个样本始终不被选中的概率为 1/e=0.368。
      2. boost思想： 每次从多数类中采样，训练模型，下次再采样时，剔除一部分被当前分类器准确分类的样本，最后融合
# 基于算法的解决方案
## 修改目标函数
    方法： 代价敏感的学习中，不同类别有不同的权重
    例子： 交叉熵L = -w1 * y * log(y) - w2 * (1 - y) * log(1 - y)
## 数量极其不平衡时
    方法： 将问题变为异常检测
    举例：
        1. 自编码， 正常数据训练一个编码-解码器，异常数据经过编码-解码后与原数据相比损失很大
        2. oneclassSVM, 用一个超球体将数据围起来，然后求最小的超球体半径，判断时，如果是在半径里，就是正常的，否则不正常
        3. Isolation Forest, 随即选择一个特征，随机选择一个分裂点，每次要对数据进行采样，采集一个小样本，预测时，统计所有树的平均高度
            起作用的假设是，异常样本属于离群点，所以容易被分到更低的叶子节点里
## 如何选择
* 正负样本都很少的情况： 应该采用数据生成的方法，扩充正负样本
* 一类数量大，一类数量极其少： 采用异常检测的办法
* 都很多的情况： 采用采样的办法
* 计算资源够，并行环境好的情况下，可以使用简单集成的办法   
