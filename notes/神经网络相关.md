# 常见激活函数以及导数
    sigmoid: 
      函数：f(z) = 1 / (1 + exp(-z))
      导数：f`(z) = f(z)(1-f(z))
    Tanh:
      函数：f(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))
      导数：f`(z) = 1 - (f(z)) ** 2
    ReLU:
      函数：f(z) = max(0, z)
      导数：f`(z) = 1 if z >= 0 else 0
# 梯度消失
    sigmoid和Tanh在z很大或者很小的时候梯度趋近于0
# Relu
  优势：
    1. 不需要计算指数
    2. 非饱和性可以解决梯度消失
    3. 单侧抑制，增加网络的表达能力
  劣势：
    1. 导致神经元死亡：负梯度在经过Relu的时候被置为0
  该进：
    Leaky Relu: f(z) = z if z > 0 else: az（a为较小的正数）
# dropout
  训练过程中随机忽略一部分神经元，测试过程中全部用到

# batchnormalization
  让数据的信息尽可能的传递下去，防止落入激活函数的饱和区域，用在输出和激活函数之间
  公式：
    x_k = (x_k - E[x_k]) / sqrt(var[x_k])
    其中：x_k为第k个神经元的原始输入，E为这批数据在第k个神经元的均值，var为标准差
    y_k = a_k * x_k + b_k
    其中： a_k, b_k为输入数据的方差和偏差，是可训练参数
# 池化操作
   均值池化：对背景保留较好，抑制方差过大
   最大池化：提取显著信息
